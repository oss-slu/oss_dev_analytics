"""
calculateHealth.py

Calculates a repository health score using lifetime repository metrics
generated by collectData.py. The health score is derived from multiple 
signals related to repository activity and efficiency and is aligned 
with the Actionable Steps and Healthy & Efficient Repository documents.
"""

import json
import os

# Configuration
DATA_PATH = "Backend/test_data.json"

# Metric weights derived from Actionable Steps research
# Weights are kept configurable so they can evolve as research changes
WEIGHTS = {
    "issue_resolution": 0.25,
    "issue_responsiveness": 0.20,
    "pr_responsiveness": 0.20,
    "contributor_activity": 0.15,
    "commit_volume": 0.20
}

# Scoring helper functions
def score_issue_resolution(rate):
    """
    Convert issue resolution rate into a normalized score
    Args:
        rate (float): Ratio of closed issues to opened issues
    Returns:
        int: Normalized score between 0 and 100
    """
    if rate >= 0.85:
        return 100
    if rate >= 0.70:
        return 80
    if rate >= 0.50:
        return 60
    if rate >= 0.30:
        return 40
    return 20

def score_time_hours(hours):
    """
    Normalize time-based metrics into a score
    Used for both issue close time and pull request merge time
    Args:
        hours (float): Time duration in hours
    Returns:
        int: Normalized score between 0 and 100
    """
    if hours <= 24:
        return 100
    if hours <= 72:
        return 80
    if hours <= 168:
        return 60
    if hours <= 336:
        return 40
    return 20

def score_contributors(count):
    """
    Scores contributor activity based on number of active contributors
    Args:
        count (int): Number of contributors with at least one commit
    Returns:
        int: Normalized score between 0 and 100
    """
    if count >= 6:
        return 100
    if count >= 4:
        return 80
    if count >= 2:
        return 60
    if count == 1:
        return 40
    return 0

def score_commits(total):
    """
    Score overall development activity using total commit count 
    Args:
        total (int): Total number of commits in the repository
    Returns:
        int: Normalized score between 0 and 100
    """
    if total >= 300:
        return 100
    if total >= 150:
        return 80
    if total >= 50:
        return 60
    if total >= 10:
        return 40
    return 20

def health_label(score):
    """
    Map numerical health score to a qualitative label
    Args:
        score (float): Final weighted health score
    Returns:
        str: Health label
    """
    if score >= 80:
        return "Healthy"
    if score >= 50:
        return "Needs Attention"
    return "Inactive"


# Metric aggregation functions
def calculate_issue_metrics(issues):
    """
    Aggregates issue-related data for a repository
    Args:
        issues (dict): Issue data grouped by contributor
    Returns:
        tuple:
            float: Issue resolution rate
            float: Average issue close time in hours
    """
    total_opened = 0
    total_closed = 0
    close_times = []

    for user_data in issues.values():
        opened = user_data.get("total_issues_opened", 0)

        # Some values are stored as strings in the JSON
        closed = int(user_data.get("total_issues_closed", 0))
        avg_time = user_data.get("average_time_to_close", 0)

        total_opened += opened
        total_closed += closed

        # Only consider contributors who actually closed issues
        if closed > 0 and avg_time > 0:
            close_times.append(avg_time)

        
    # Resolution rate indicates how well issues are being handled overall
    resolution_rate = (
        total_closed / total_opened if total_opened > 0 else 0
    )

    # If no issues were closed, treat responsiveness as very poor
    avg_close_time = (
        sum(close_times) / len(close_times) if close_times else float("inf")
    )

    return resolution_rate, avg_close_time

def calculate_pr_metrics(prs):
    """
    Aggregate pull request data for a repository
    Args:
        prs (dict): Pull request data grouped by contributor
    Returns:
        float: Average pull request merge time in hours
    """
    merge_times = []

    for user_data in prs.values():
        avg_time = user_data.get("average_time_to_merge", 0)

        # Ignore entries with no merge data
        if avg_time > 0:
            merge_times.append(avg_time)

    # Long or missing merge times indicate workflow friction
    avg_merge_time = (
        sum(merge_times) / len(merge_times) if merge_times else float("inf")
    )

    return avg_merge_time

def calculate_commit_metrics(commits):
    """
    Aggregate commit data for a repository
    Args:
        commits (dict): Commit data grouped by contributor
    Returns:
        tuple:
            int: Total number of commits
            int: Number of active contributors
    """
    total_commits = 0
    active_contributors = 0

    for user_data in commits.values():
        count = user_data.get("total_commits", 0)
        total_commits += count

        # Any contributor with at least one commit is considered active
        if count > 0:
            active_contributors += 1

    return total_commits, active_contributors

# Health score calculation
def calculate_health_scores(data):
    """
    Calculates health scores for all repositories using lifetime data
    Sprint-level repository entries are ignored
    Args:
        data (dict): Parsed JSON data from test_data.json
    Returns:
        dict: Health scores and metrics per repository
    """
    results = {}

    for repo_name, repo_data in data.items():
        # Skip sprint-specific data for lifetime health evaluation
        if "_sprint_" in repo_name:
            continue

        issues = repo_data.get("issues", {})
        prs = repo_data.get("pull_requests", {})
        commits = repo_data.get("commits", {})

        # Computing raw metrics
        issue_rate, issue_time = calculate_issue_metrics(issues)
        pr_time = calculate_pr_metrics(prs)
        total_commmits, contributors = calculate_commit_metrics(commits)

        # Converting raw metrics into normalized scores
        metric_scores = {
            "issue_resolution": score_issue_resolution(issue_rate),
            "issue_responsiveness": score_time_hours(issue_time),
            "pr_responsiveness": score_time_hours(pr_time),
            "contributor_activity": score_contributors(contributors),
            "commit_volume": score_commits(total_commmits),
        }

        # Final weighted health score
        final_score = round(
            sum(metric_scores[m] * WEIGHTS[m] for m in WEIGHTS), 2
        )

        results[repo_name] = {
            "metrics": metric_scores,
            "final_score": final_score,
            "status": health_label(final_score)
        }
    
    return results

# Script entry point
if __name__ == "__main__":
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError("test_data.json not found")
    
    with open(DATA_PATH, "r") as f:
        data = json.load(f)

    health_scores = calculate_health_scores(data)

    # Output is printed in JSON format for easy validation a
    # and future integration with the React frontend
    print(json.dumps(health_scores, indent=4))