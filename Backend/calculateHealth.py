import json
import os

# Configuration
# This script consumes lifetime repository data generated by collectData.py
# Sprint-level data is intentionally ignored for health score
DATA_PATH = "Backend/test_data.json"

# Metric weights are based on the Actionable Steps and the "Healthy and Efficient Repository" research document
# The goal is to balance repository activity (health) with responsiveness and workflow activity (efficiency)
WEIGHTS = {
    "issue_resolution": 0.25,
    "issue_responsiveness": 0.20,
    "pr_responsiveness": 0.20,
    "contributor_activity": 0.15,
    "commit_volume": 0.20
}

# Scoring helper functions
# These helpers normalize raw values into a 0-100 scale so that different metrics can be combined fairly
def score_issue_resolution(rate):
    # Measures how effectively issues are being closed
    if rate >= 0.85:
        return 100
    if rate >= 0.70:
        return 80
    if rate >= 0.50:
        return 60
    if rate >= 0.30:
        return 40
    return 20

def score_time_hours(hours):
    # Used for both issue close time and PR merge time
    # Faster response times generally indicate better efficiency
    if hours <= 24:
        return 100
    if hours <= 72:
        return 80
    if hours <= 168:
        return 60
    if hours <= 336:
        return 40
    return 20

def score_contributors(count):
    # Reflects how distributed and collaborative the work is
    if count >= 6:
        return 100
    if count >= 4:
        return 80
    if count >= 2:
        return 60
    if count == 1:
        return 40
    return 0

def score_commits(total):
    # Acts as a rough indicator of overall development activity
    if total >= 300:
        return 100
    if total >= 150:
        return 80
    if total >= 50:
        return 60
    if total >= 10:
        return 40
    return 20

def health_label(score):
    # Qualitative labels make the score easier to interpret
    # These labels are meant to guide attention, not judge quality
    if score >= 80:
        return "Healthy"
    if score >= 50:
        return "Needs Attention"
    return "Inactive"


# Metric aggregation (repo-level)
# These functions aggregate contributor-level data into meaningful repository-wide metrics
def calculate_issue_metrics(issues):
    total_opened = 0
    total_closed = 0
    close_times = []

    for user_data in issues.values():
        opened = user_data.get("total_issues_opened", 0)

        # Some fields are stored as strings in the JSON
        closed = int(user_data.get("total_issues_closed", 0))
        avg_time = user_data.get("average_time_to_close", 0)

        total_opened += opened
        total_closed += closed

        # Only consider contributors who actually closed issues
        if closed > 0 and avg_time > 0:
            close_times.append(avg_time)

        
    # Resolution rate indicates how well issues are being handled overall
    resolution_rate = (
        total_closed / total_opened if total_opened > 0 else 0
    )

    # If no issues were closed, treat responsiveness as very poor
    avg_close_time = (
        sum(close_times) / len(close_times) if close_times else float("inf")
    )

    return resolution_rate, avg_close_time

def calculate_pr_metrics(prs):
    merge_times = []

    for user_data in prs.values():
        avg_time = user_data.get("average_time_to_merge", 0)

        # Ignore entries with no merge data
        if avg_time > 0:
            merge_times.append(avg_time)

    # Long or missing merge times indicate workflow friction
    avg_merge_time = (
        sum(merge_times) / len(merge_times) if merge_times else float("inf")
    )

    return avg_merge_time

def calculate_commit_metrics(commits):
    total_commits = 0
    active_contributors = 0

    for user_data in commits.values():
        count = user_data.get("total_commits", 0)
        total_commits += count

        # Any contributor with at least one commit is considered active
        if count > 0:
            active_contributors += 1

    return total_commits, active_contributors

# Health score calculation
def calculate_health_scores(data):
    results = {}

    for repo_name, repo_data in data.items():
        # Skip sprint-specific data for lifetime health evaluation
        if "_sprint_" in repo_name:
            continue

        issues = repo_data.get("issues", {})
        prs = repo_data.get("pull_requests", {})
        commits = repo_data.get("commits", {})

        # Computing raw metrics
        issue_rate, issue_time = calculate_issue_metrics(issues)
        pr_time = calculate_pr_metrics(prs)
        total_commmits, contributors = calculate_commit_metrics(commits)

        # Converting raw metrics into normalized scores
        metric_scores = {
            "issue_resolution": score_issue_resolution(issue_rate),
            "issue_responsiveness": score_time_hours(issue_time),
            "pr_responsiveness": score_time_hours(pr_time),
            "contributor_activity": score_contributors(contributors),
            "commit_volume": score_commits(total_commmits),
        }

        # Final weighted health score
        final_score = round(
            sum(metric_scores[m] * WEIGHTS[m] for m in WEIGHTS), 2
        )

        results[repo_name] = {
            "metrics": metric_scores,
            "final_score": final_score,
            "status": health_label(final_score)
        }
    
    return results

# Script entry point
if __name__ == "__main__":
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError("test_data.json not found")
    
    with open(DATA_PATH, "r") as f:
        data = json.load(f)

    health_scores = calculate_health_scores(data)

    # Output is printed in JSON format for easy validation and future integration with the React frontend
    print(json.dumps(health_scores, indent=4))